---
title: "Predicting time series"
author: "Thomas Nauss"
date: "19 Oktober 2017"
output: 
  html_document: 
    keep_md: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path='{{ site.baseurl }}/assets/images/rmd_images/e09-03/')
library(envimaR)
root_folder = alternativeEnvi(root_folder = "~/edu/mpg-data-analysis/", 
                              alt_env_id = "COMPUTERNAME",
                              alt_env_value = "PCRZP", 
                              alt_env_root_folder = "F:\\BEN\\edu")
source(file.path(root_folder, "moer-mpg-data-analysis/staging/examples/000_set_environment.R"))
```
Time-series analysis can generally be divided into forecasting future dynamics or describing and potentially explaining past patterns. Since the later often requires continous i.e. gap-free observations, we start with a simple forecasting procedure which can also be used for gap-filling in some cases.

To illustrate the forecasting, we will again use the (mean monthly) air temperature record of the weather station in Coelbe (which is closest to the Marburg university forest). The data has been supplied by the German weatherservice [German weather service](ftp://ftp-cdc.dwd.de/pub/CDC/observations_germany/). 

```{r, warning=FALSE, echo=FALSE}
path_dwd <- file.path(envrmt$path_dwd,"3164_coelbe/")

dwd <- read.table(file.path(path_dwd, "produkt_temp_Terminwerte_20060701_20151231_03164.txt"),
                  header = TRUE, sep = ";", dec = ".")
dwd$AGG_JM <- substr(dwd$MESS_DATUM, 1, 6)
tam <- aggregate(dwd$LUFTTEMPERATUR, by = list(dwd$AGG_JM), FUN = mean)
colnames(tam) <- c("Date", "Ta")
```

```{r, warning=FALSE}
head(tam)
```

### Auto-regressive and moving average models for predicting time series
In order to predict the time series just based on its observed dynamics, an auto-regressive inegrated moving average model (ARIMA) can be used. ARIMA is not a diagnostic model for the identification e.g. of seasonal components etc. On the contrary, it requires a stationary time series which we alread know is not the case for the monthly mean air temperature values between July 1st 2006 and December 31st 2015. 
```{r, warning=FALSE}
acf(tam$Ta)
```

### Auto-regressive models (AR)
While we can start with detrending and de-seasoning now, let's try a quick-and-dirty approach first and postbone the time series decomposition to the next sessiong. One thing one could try is to use not the original values but the difference between the consecutive value pairs (i.e. the difference between time t and t+1 etc.). And if one difference is not enough, we can compute the difference of the differences and the auto-correlation behaviour would look like that:
```{r, warning=FALSE}
dTa <- diff(diff(tam$Ta))
acf(dTa)
```

Not perfect but good enough to be used in this example. Before we run the ARIMA, we run a simple auto-regressive (AR) model first:
```{r, warning=FALSE}
armod <- ar(dTa, aic = TRUE, order.max = 20, method = "yule-walker")
armod
```

The ``ar`` function computes an AR Model and iterates over the lags to be included in the linear equation which predicts the monthly air temperature value for time t based on its value at time t-1, t-2, .... 

The maximum number of lags (within a maximum of 20 as provided to the function) is determined based on an internal estimate of the AIC parameter. The number of lags with the minimum AIC are used:
```{r, warning=FALSE}
plot(0:20, armod$aic, type = "o")
```

Using the Yule-Walker method for estimating the AR parameters, a maximum lag of 10 is considered. Although the results will be quite different, if another method is used and although the following is of no use if one actually wants to use not just an auto-regressive (AR) or a moving average model (MA) but an ARIMA, a rule of thump states that if

* the auto-correlation function declines exponentially or shows a sinus pattern and
* the partial auto-correlation function shows only p significant lags in the beginning,

an AR(p) model (i.e. an AR model with p considered lags) is a good starting point. For moving average models it is vice versa and the lags q considered in the model would be taken from the auto-correlation function. In the auto- and partial auto-correlation function of the two-timdes differentiated dataset, we might see exatly this:
```{r, warning=FALSE}
par_org <- par()
par(mfrow = c(1,2))
acf(dTa)
pacf(dTa)
par(par_org)
```

Nice, but generally although pretty much useless since if we change the parameter estimation method, the results look like that (17 is the optiomum):
```{r, warning=FALSE}
armod <- ar(dTa, aic = TRUE, order.max = 20, method = "mle")
armod
```


The AR model can now be used to predict the time series (although 100 month in the future is way to much; grey lines indicate the standard error):
```{r, warning=FALSE}
arpred <- predict(armod, n.ahead = 100)

plot(dTa, type = "l", xlim = c(0, length(dTa)+100))
lines(arpred$pred, col = "red")
lines(arpred$pred + arpred$se, col = "grey")
lines(arpred$pred - arpred$se, col = "grey")
```


### Auto-regressive integrated moving average models (ARIMA)
Since the auto-correlation and partial auto-correlation function might slightly demand for an auto-regressive model and not a moving average model, we will skip the later and jump directly to the combined version - ARIMA.

In the ARIMA world, three parameters are generally required: 

* p which is the number of time lags considered in the AR model
* d which is the number of differnces taken from the dataset (to reach stationarity)
* q which is the number of time lags considered in the MA model

Hence, if we want to compute an ARIMA compareable to the AR above (the second one), we call it with:
```{r, warning=FALSE}
armod <- arima(tam$Ta, order = c(17,2,0), method = "ML")  # the order is p, d, q
armod
```

Finding the right values for an arima model is not so easy. Basically, one has to iterate over a variety of options. Let's try just one for illustration purposes.
```{r, warning=FALSE}
arimamod <- arima(tam$Ta, c(6,2,2))
summary(arimamod)
```

We will not focus on optimal prediction in this example, so these results are just fine for the purpose of illustration. The arima model can be used to predict the time series in the future analogously to the AR model:
```{r, warning=FALSE}
arima_predict <- predict(arimamod, n.ahead = 100)

plot(tam$Ta, type = "l", xlim = c(0, length(dTa)+100))
lines(arima_predict$pred, col = "red")
lines(arima_predict$pred + arima_predict$se, col = "grey")
lines(arima_predict$pred - arima_predict$se, col = "grey")
```


### Finding the right parameters for ARIMA
Finding the right number of lags p for the AR model, number of differentiations d, and number of lags q for the MA model requires some iteration. You now already everything you need to e.g. iterate over a bunch of values for each parameter and select the appropriate combination by minimizing e.g. cross-validation errors. An alternative approach could be the ``forecast::auto.arima`` function. It will iterate over the parameters (see defaults of the function) and apply some statistical tests for stationarity etc. It will also inlcude seasonal components in the ARIMA (wich is basically the same as a time lag but with respect to the seasonal difference, e.g. a seasonal component [12] with a lag of 1 skips 12 month, one for [4] skips a quater of a year).

### Finding the right parameters for ARIMA with the auto.arima function
The forecast package generally works on time series data of class ts which is easy to generate if you actually have a complete (i.e. gap-free) time series. The time series is defined by the values, the starting point in time (e.g. year and month), the end point in time, the frequency of obsrevations per main time unit and the time step deltat. For monthly data starting in July 2006 and ending in December 2015, the creation of a time series will look like this:
```{r, warning=FALSE}
tam_ts <- ts(tam$Ta, start = c(2006, 7), end = c(2015, 12), 
             frequency = 12)
str(tam_ts)
```

Now, we can use the ``forecast::auto.arima`` function. We will extend the iteration over p and q to 20 to be compatible with the above example but not include a seasonal component for the moment:
```{r, warning=FALSE}
# library(forecast)
arima_ns <- auto.arima(tam_ts, max.p = 20, max.q = 20, stationary = TRUE, seasonal=FALSE)
summary(arima_ns)
```

Actually, the arima comes with d = 0 whic means that the data values is not differentiated (which is a tough choice).

To better estimate the quality of the ARIMA, we can use some diagnostic plots:
```{r, warning=FALSE}
tsdiag(arima_ns)
```

The last plot shows test results if the residuals are independently distributed. For time lags larger 4, this is maybe not the case which indicates that the arima model is not really well specified.

The forecast will look like this:
```{r, warning=FALSE}
plot(forecast(arima_ns))
```

If we allow a seasonal component, the results look like that:
```{r, warning=FALSE}
arima_s <- auto.arima(tam_ts, max.p = 20, max.q = 20, seasonal=TRUE)
summary(arima_s)
tsdiag(arima_s)
plot(forecast(arima_s))
```
