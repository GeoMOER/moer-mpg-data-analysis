---
title: "Cross-validation"
author: "Thomas Nauss"
date: "19 Oktober 2017"
output: 
  html_document: 
    keep_md: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path='{{ site.baseurl }}/assets/images/rmd_images/e06-02/')
library(envimaR)
root_folder = alternativeEnvi(root_folder = "~/edu/mpg-data-analysis/", 
                              alt_env_id = "COMPUTERNAME",
                              alt_env_value = "PCRZP", 
                              alt_env_root_folder = "F:\\BEN\\edu")
source(file.path(root_folder, "moer-mpg-data-analysis/staging/examples/000_set_environment.R"))

```
Test statistics can describe the quality or accuracy of regression models if the assumptions of the models are met. However, the assessment would still be based on a model that is perfectly fitted to a given sample. To assess the prediction performance of a model in a more independent manner, the accuracy must be computed based on an independent (sub)-sample.

The straight-forward way for a test on an independent sample is of course the one which actually fits the model on sample 1 and tests it on a completely different sample 2. However, in real-world applications, the required sample size is not givin in many cases. Therefore, cross-validation - althgough not entierly independent - is a good alternative to get an idea of the model performance for data values which have not been part of the model fitting. In principal, there are to strategies for cross-validation:

* Leave-one-out cross-validation: in this case, one value pair of the sample data set is left out during the model fitting and the model accuracy is estimated based on the quality of the prediction of the left-out value. In order to get a sufficient sample size, this procedure is iterated over all value pairs of the data set (i.e. each value is left out once).
* Leave-many-out cross-validation: in this case, more than one value pair of the sample data set is left out during the model fitting and the model accuracy is estimated based on the quality of the prediction of the left-out samples. This strategy is good for larger data sets, where e.g. 80% of the data can be used as training data for fitting the and 20% can be used as independent validation data. The procedure could of course be repeated by creating different training data sets by chance and testing them using the respective left-out samples. Please note that in the latter case, the independency is compromised to a small degree. On the other hand, one get's a better impression of the model performance especially if the different validation data sets are not averaged but used independently in order to get an idea of the performance variation.

To illustrate this concept, we stay with the [anscombe dataset](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/anscombe.html) and use the variables x1 and y1 as independent and dependent variables.

The statistics of a linear regression model would be:
```{r, warning=FALSE}
lmod <- lm(y1 ~ x1, data = anscombe)

plot(anscombe$x1, anscombe$y1)
abline(lmod, col = "red")

anova(lmod)
summary(lmod)
```

### Leave-one-out cross-validation
For the leave-one-out cross-validation, the training sample is comprised by all but one value pairs of the data set and the left-out data pair is used as validation sample. The procedure is typically iterated over the entire data set, i.e. each value is left-out once.
```{r, warning=FALSE}
# Leave one out cross-validation
cv <- lapply(seq(nrow(anscombe)), function(i){
  train <- anscombe[-i,]
  test <- anscombe[i,]
  lmod <- lm(y1 ~ x1, data = train)
  pred <- predict(lmod, newdata = test)
  obsv <- test$y1
  data.frame(pred = pred,
             obsv = obsv,
             model_r_squared = summary(lmod)$r.squared)
})
cv <- do.call("rbind", cv)

ss_obsrv <- sum((cv$obsv - mean(cv$obsv))**2)
ss_model <- sum((cv$pred - mean(cv$obsv))**2)
ss_resid <- sum((cv$obsv - cv$pred)**2)

mss_obsrv <- ss_obsrv / (length(cv$obsv) - 1)
mss_model <- ss_model / 1
mss_resid <- ss_resid / (length(cv$obsv) - 2)

plot(cv$obsv, (cv$obsv - cv$pred))
```

Based on the above computation, one could compute something like the F value also used within the anova and estimate an r square. As compared to the "internal" F test of the anove from the original model and the r square, the cross-validation indicates a less accurate prediction.
```{r, warning=FALSE}
data.frame(NAME = c("cross-validation F value",
                    "linear model F value", 
                    "cross-validatino r squared",
                    "lienar model r squared"),
           VALUE = c(round(mss_model / mss_resid, 2),
                     round(anova(lmod)$'F value'[1], 2),
                     round(1 - ss_resid / ss_obsrv, 2),
                     round(summary(lmod)$r.squared, 2)))

print("Range of r squared from the individual models computed in the cv:")
summary(cv$model_r_squared)
```

Aside from that, a variety of different errors is commonly used to describe the predictive performance of the model (for regression based approaches):
```{r, warning=FALSE}
se <- function(x) sd(x, na.rm = TRUE)/sqrt(length(na.exclude(x)))

me <- round(mean(cv$pred - cv$obs, na.rm = TRUE), 2)
me_sd <- round(se(cv$pred - cv$obs), 2)
mae <- round(mean(abs(cv$pred - cv$obs), na.rm = TRUE), 2)
mae_sd <- round(se(abs(cv$pred - cv$obs)), 2)
rmse <- round(sqrt(mean((cv$pred - cv$obs)^2, na.rm = TRUE)), 2)
rmse_sd <- round(se((cv$pred - cv$obs)^2), 2)

data.frame(NAME = c("Mean error (ME)", "Std. error of ME", 
                    "Mean absolute error (MAE)", "Std. error of MAE", 
                    "Root mean square error (RMSE)", "Std. error of RMSE"),
           VALUE = c(me, me_sd,
                     mae, mae_sd,
                     rmse, rmse_sd))
```


### Leave-many-out cross-validation
For the leave-many-out cross-validation, the following example computes 100 different models. For each model, a training sample of 80% of the data set is choosen randomly.

```{r, warning=FALSE}
range <- nrow(anscombe)
nbr <- nrow(anscombe) * 0.8

cv_sample <- lapply(seq(100), function(i){
  set.seed(i)
  smpl <- sample(range, nbr)
  train <- anscombe[smpl,]
  test <- anscombe[-smpl,]
  lmod <- lm(y1 ~ x1, data = train)
  pred <- predict(lmod, newdata = test)
  obsv <- test$y1
  resid <- obsv-pred
  ss_obsrv <- sum((obsv - mean(obsv))**2)
  ss_model <- sum((pred - mean(obsv))**2)
  ss_resid <- sum((obsv - pred)**2)
  mss_obsrv <- ss_obsrv / (length(obsv) - 1)
  mss_model <- ss_model / 1
  mss_resid <- ss_resid / (length(obsv) - 2)
  data.frame(pred = pred,
             obsv = obsv,
             resid = resid,
             ss_obsrv = ss_obsrv,
             ss_model = ss_model,
             ss_resid = ss_resid,
             mss_obsrv = mss_obsrv,
             mss_model = mss_model,
             mss_resid = mss_resid,
             r_squared = ss_model / ss_obsrv
  )
})
cv_sample <- do.call("rbind", cv_sample)

ss_obsrv <- sum((cv_sample$obsv - mean(cv_sample$obsv))**2)
ss_model <- sum((cv_sample$pred - mean(cv_sample$obsv))**2)
ss_resid <- sum((cv_sample$obsv - cv_sample$pred)**2)

mss_obsrv <- ss_obsrv / (length(cv_sample$obsv) - 1)
mss_model <- ss_model / 1
mss_resid <- ss_resid / (length(cv_sample$obsv) - 2)
```

Again, an overview of other errors used to descirbe the prediction performance:
```{r, warning=FALSE}
data.frame(NAME = c("cross-validation F value",
                    "linear model F value", 
                    "cross-validatino r squared",
                    "lienar model r squared"),
           VALUE = c(round(mss_model / mss_resid, 2),
                     round(anova(lmod)$'F value'[1], 2),
                     round(1 - ss_resid / ss_obsrv, 2),
                     round(summary(lmod)$r.squared, 2)))

print("Range of r squared from the individual models computed in the cv:")
summary(cv$model_r_squared)
```

Aside from that, a variety of different errors is commonly used to describe the predictive performance of the model (for regression based approaches):
```{r, warning=FALSE}
se <- function(x) sd(x, na.rm = TRUE)/sqrt(length(na.exclude(x)))

me <- round(mean(cv_sample$pred - cv_sample$obs, na.rm = TRUE), 2)
me_sd <- round(se(cv_sample$pred - cv_sample$obs), 2)
mae <- round(mean(abs(cv_sample$pred - cv_sample$obs), na.rm = TRUE), 2)
mae_sd <- round(se(abs(cv_sample$pred - cv_sample$obs)), 2)
rmse <- round(sqrt(mean((cv_sample$pred - cv_sample$obs)^2, na.rm = TRUE)), 2)
rmse_sd <- round(se((cv_sample$pred - cv_sample$obs)^2), 2)

data.frame(NAME = c("Mean error (ME)", "Std. error of ME", 
                    "Mean absolute error (MAE)", "Std. error of MAE", 
                    "Root mean square error (RMSE)", "Std. error of RMSE"),
           VALUE = c(me, me_sd,
                     mae, mae_sd,
                     rmse, rmse_sd))
```
